<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="r-VnR_9WEe-TelKdzkrFWPN779jN-sZhatKeISSLq2U">
  <meta name="msvalidate.01" content="BD9404A1FE093987092D7FA9ACAF4EAA">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"physics-lee.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.18.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="See the pdf version here! The Big PictureAI and MLIt is hard to give Artificial Intelligence (AI) a precise definition. In reality, many experts define it as: to make computers be like humans. This wo">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning">
<meta property="og:url" content="https://physics-lee.github.io/2023/02/26/Machine_Learning/index.html">
<meta property="og:site_name" content="Physics-Lee&#39;s Land">
<meta property="og:description" content="See the pdf version here! The Big PictureAI and MLIt is hard to give Artificial Intelligence (AI) a precise definition. In reality, many experts define it as: to make computers be like humans. This wo">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="c:\Users\11097\AppData\Roaming\Typora\typora-user-images\image-20230226195805734.png">
<meta property="og:image" content="c:\Users\11097\AppData\Roaming\Typora\typora-user-images\image-20230226212632493.png">
<meta property="og:image" content="c:\Users\11097\AppData\Roaming\Typora\typora-user-images\image-20230226221617505.png">
<meta property="og:image" content="c:\Users\11097\AppData\Roaming\Typora\typora-user-images\image-20230227212715725.png">
<meta property="og:image" content="c:\Users\11097\AppData\Roaming\Typora\typora-user-images\image-20230227232313535.png">
<meta property="article:published_time" content="2023-02-25T16:00:00.000Z">
<meta property="article:modified_time" content="2023-12-22T14:40:14.000Z">
<meta property="article:author" content="Physics-Lee">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="c:\Users\11097\AppData\Roaming\Typora\typora-user-images\image-20230226195805734.png">


<link rel="canonical" href="https://physics-lee.github.io/2023/02/26/Machine_Learning/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://physics-lee.github.io/2023/02/26/Machine_Learning/","path":"2023/02/26/Machine_Learning/","title":"Machine Learning"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Machine Learning | Physics-Lee's Land</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-B7TPY4HDXE"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-B7TPY4HDXE","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?d2fddd04f380f60410824da431ddee80"></script>







  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Physics-Lee's Land" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Physics-Lee's Land</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-games"><a href="/Games/" rel="section"><i class="fa fa-sailboat fa-fw"></i>Games</a></li><li class="menu-item menu-item-compu_neuro"><a href="/Compu_Neuro/" rel="section"><i class="fa-solid fa-code-branch fa-fw"></i>Compu_Neuro</a></li><li class="menu-item menu-item-rss"><a href="/atom.xml" rel="section"><i class="fa fa-rss fa-fw"></i>RSS</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#The-Big-Picture"><span class="nav-number">1.</span> <span class="nav-text">The Big Picture</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#AI-and-ML"><span class="nav-number">1.1.</span> <span class="nav-text">AI and ML</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%91%E7%9D%A3%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6"><span class="nav-number">1.2.</span> <span class="nav-text">监督式学习的基本框架</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A5%BF%E7%93%9C%E4%B9%A6"><span class="nav-number">1.3.</span> <span class="nav-text">西瓜书</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CH1"><span class="nav-number">2.</span> <span class="nav-text">CH1</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CH2"><span class="nav-number">3.</span> <span class="nav-text">CH2</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CH3-Linear-Model"><span class="nav-number">4.</span> <span class="nav-text">CH3 Linear Model</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Normal-Linear-Regression"><span class="nav-number">4.1.</span> <span class="nav-text">Normal Linear Regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Generalized-Linear-Model"><span class="nav-number">4.2.</span> <span class="nav-text">Generalized Linear Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#From-Regression-to-Classification"><span class="nav-number">4.3.</span> <span class="nav-text">From Regression to Classification</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Logistic-Regression"><span class="nav-number">4.4.</span> <span class="nav-text">Logistic Regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LASSO-and-Ridge-Regression"><span class="nav-number">4.5.</span> <span class="nav-text">LASSO and Ridge Regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PCA"><span class="nav-number">4.6.</span> <span class="nav-text">PCA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LDA"><span class="nav-number">4.7.</span> <span class="nav-text">LDA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Supporting-Vector-Machine"><span class="nav-number">4.8.</span> <span class="nav-text">Supporting Vector Machine</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E5%88%86%E7%B1%BB%E6%8B%93%E5%B1%95%E5%88%B0%E5%A4%9A%E5%88%86%E7%B1%BB"><span class="nav-number">4.9.</span> <span class="nav-text">二分类拓展到多分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%B1%BB%E5%88%AB%E4%B8%8D%E5%B9%B3%E8%A1%A1"><span class="nav-number">4.10.</span> <span class="nav-text">类别不平衡</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CH4-Decision-Tree"><span class="nav-number">5.</span> <span class="nav-text">CH4 Decision Tree</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CH5-Neural-Network"><span class="nav-number">6.</span> <span class="nav-text">CH5 Neural Network</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CH8-Ensemble-Learning"><span class="nav-number">7.</span> <span class="nav-text">CH8 Ensemble Learning</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CH9-Clustering"><span class="nav-number">8.</span> <span class="nav-text">CH9 Clustering</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CH10-Dimensionality-Reduction"><span class="nav-number">9.</span> <span class="nav-text">CH10 Dimensionality Reduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CH11-Feature-Selection"><span class="nav-number">10.</span> <span class="nav-text">CH11 Feature Selection</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CH13-%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">11.</span> <span class="nav-text">CH13 半监督学习</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CH7-Bayes"><span class="nav-number">12.</span> <span class="nav-text">CH7 Bayes</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CH14-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B"><span class="nav-number">13.</span> <span class="nav-text">CH14 概率图模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-number">14.</span> <span class="nav-text">附录</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Normal-Linear-Regression%E7%9A%84%E8%A7%A3%E6%9E%90%E8%A7%A3"><span class="nav-number">14.1.</span> <span class="nav-text">Normal Linear Regression的解析解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Logistic-Regression%E7%9A%84%E6%A6%82%E7%8E%87%E5%90%AB%E4%B9%89%E5%92%8CE%E7%9A%84%E6%8E%A8%E5%AF%BC"><span class="nav-number">14.2.</span> <span class="nav-text">Logistic Regression的概率含义和E的推导</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%87%92%E6%83%B0%E5%AD%A6%E4%B9%A0-vs-%E6%80%A5%E5%88%87%E5%AD%A6%E4%B9%A0"><span class="nav-number">14.3.</span> <span class="nav-text">懒惰学习 vs 急切学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E4%BA%9B%E5%B8%B8%E8%A7%81%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">14.4.</span> <span class="nav-text">一些常见的数据集</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Physics-Lee</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">403</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">82</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">65</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Physics-Lee" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Physics-Lee" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/Yixuan_Li_USTC" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;Yixuan_Li_USTC" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.youtube.com/@XuanJr." title="YouTube → https:&#x2F;&#x2F;www.youtube.com&#x2F;@XuanJr." rel="noopener me" target="_blank"><i class="fab fa-youtube fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://math.stackexchange.com/users/1247222/bruce-lee-of-ustc" title="MathOverflow → https:&#x2F;&#x2F;math.stackexchange.com&#x2F;users&#x2F;1247222&#x2F;bruce-lee-of-ustc" rel="noopener me" target="_blank"><i class="fa-regular fa-paper-plane fa-fw"></i></a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://physics-lee.github.io/2023/02/26/Machine_Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Physics-Lee">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Physics-Lee's Land">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Machine Learning | Physics-Lee's Land">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Machine Learning
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-02-26 00:00:00" itemprop="dateCreated datePublished" datetime="2023-02-26T00:00:00+08:00">2023-02-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/2023/" itemprop="url" rel="index"><span itemprop="name">2023</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/2023/2023-02/" itemprop="url" rel="index"><span itemprop="name">2023-02</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>4.6k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>15 mins.</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>See <a href="https://physics-lee.github.io/pdfs/Machine_Learning.pdf">the pdf version</a> here!</p>
<h1 id="The-Big-Picture"><a href="#The-Big-Picture" class="headerlink" title="The Big Picture"></a>The Big Picture</h1><h2 id="AI-and-ML"><a href="#AI-and-ML" class="headerlink" title="AI and ML"></a>AI and ML</h2><p>It is hard to give Artificial Intelligence (AI) a precise definition. In reality, many experts define it as: <strong>to make computers be like humans</strong>. This word, AI, was first proposed in 1956 in Dartmouth summer workshop.</p>
<p>AI has 3 very popular branches</p>
<ul>
<li>Machine Learning (ML): <strong>to make computers learn like humans</strong><ul>
<li>A practical definition of learning: learning is a process where a system improves performance from experience. This definition is given by Herbert Simon (司马贺).</li>
</ul>
</li>
<li>Computer Vision (CV): to train computers to interpret and understand the visual world like humans</li>
<li>Natural Language Processing (NLP): giving computers the ability to understand text and spoken words like humans</li>
</ul>
<p>ML can be divided into 4 major types</p>
<ul>
<li>supervised learning: have human tags</li>
<li>unsupervised learning: don’t have human tags</li>
<li>reinforce learning: learn from reward</li>
<li>artificial neural network&#x2F;deep learning: use artificial neural network</li>
</ul>
<p>Some people regard reinforce learning and deep learning as same-level-concept to ML, not its subset.</p>
<p>Summary:</p>
<img src="C:\Users\11097\AppData\Roaming\Typora\typora-user-images\image-20230226195805734.png" alt="image-20230226195805734" style="zoom:50%;" />

<h2 id="监督式学习的基本框架"><a href="#监督式学习的基本框架" class="headerlink" title="监督式学习的基本框架"></a>监督式学习的基本框架</h2><p>机器学习、深度学习、强化学习中都是监督式学习最常见，下面是它的基本框架。</p>
<p>数据集D中有n个点和它们的标签：<br>$$<br>(\vec{x_1},y_1), (\vec{x_2},y_2),…,(\vec{x_n},y_n)<br>$$</p>
<p>每个点有p个特征<br>$$<br>\vec{x_i} &#x3D; \begin{bmatrix}<br>x_{i1} \<br>x_{i2} \<br>\vdots \<br>x_{ip}<br>\end{bmatrix}<br>$$</p>
<p>在lab5中，n&#x3D;10000，p&#x3D;120。</p>
<p>当y离散时，称之为分类问题。</p>
<p>当y连续时，称之为回归问题。</p>
<p>之后我都将使用这些记号。</p>
<h2 id="西瓜书"><a href="#西瓜书" class="headerlink" title="西瓜书"></a>西瓜书</h2><p>CH1、CH2介绍了机器学习的big picture。</p>
<p>CH5简单介绍了深度学习。</p>
<p>CH16简单介绍了强化学习。</p>
<p>这里插一句，AlphaGo和AlphaFold用的方法都是深度学习和强化学习的结合，取得了巨大的成功。</p>
<p>有很多章节介绍了supervised learning</p>
<ul>
<li>CH3：线性模型</li>
<li>CH6：支持向量机（其实支持向量机也是一种线性模型）</li>
<li>CH4：决策树</li>
</ul>
<p>有一个章节介绍了unsupervised learning</p>
<ul>
<li>CH9：聚类</li>
</ul>
<p>有两个章节介绍了如何把机器学习看成贝叶斯框架下的东西（机器学习中的很多定理、算法都可以转化到贝叶斯框架下，从而有一些贝叶斯框架下的含义；或者可以转化到概率框架下，有一些概率含义；比如上次我发给你的RELIEF F算法，最开始提出时只是提出者偶然发现它好用，后来的很多研究者给了这个算法一种概率含义。）</p>
<ul>
<li>CH7：贝叶斯分类器</li>
<li>CH14：概率图模型</li>
</ul>
<p>CH8介绍了集成学习。集成学习可以把一些弱学习器结合起来获得更好的效果。</p>
<p>CH10介绍了降维、CH11介绍了特征选择。现实中，降维或特征选择非常常用。</p>
<p>CH13介绍了半监督学习。</p>
<p>CH12、CH15，我没有仔细看。</p>
<p>接下来我按照章节顺序写summary。</p>
<h1 id="CH1"><a href="#CH1" class="headerlink" title="CH1"></a>CH1</h1><ul>
<li>Occam’s razor: 若有多个假设与观察一致,则选最简单的那个；如无必要，勿增实体。  </li>
<li>No Free Lunch Theorem: Any two optimization algorithms are equivalent when their performance is averaged across all possible problems.</li>
</ul>
<h1 id="CH2"><a href="#CH2" class="headerlink" title="CH2"></a>CH2</h1><ul>
<li>generalization ability (泛化能力): 指一个模型对于训练集之外的数据进行预测的能力。</li>
<li>bias-variance-tradeoff (偏差方差权衡): $MSE &#x3D; Bias + Variance + Noise$。<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">维基百科</a> 的公式推导写得很好。bias-variance-tradeoff和欠拟合、过拟合有直接关系，如下图所示，欠拟合时bias太大，过拟合时variance太大。因此，我们要让模型有合适的复杂度，在bias和variance中进行权衡。</li>
</ul>
<img src="C:\Users\11097\AppData\Roaming\Typora\typora-user-images\image-20230226212632493.png" alt="image-20230226212632493" style="zoom:50%;" />

<ul>
<li><p>performance measure</p>
<ul>
<li><p>regression: MSE</p>
</li>
<li><p>classification</p>
<ul>
<li><p><strong>accuracy (最先考察的肯定是accuracy，随后才考察下面的那些。)</strong></p>
</li>
<li><p>precision, recall, Precision-Recall graph</p>
</li>
<li><p>ROC, AUC</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>hypothesis test: 比较不同模型的泛化能力。</p>
</li>
<li><p>划分数据集的方式：留出法（要做多次取平均，比如，按6：2：2随机划分训练集、验证集、测试集，重复100次）、交叉验证法（推荐10次10折交叉验证）、bootstrapping（在集成学习里常用）</p>
</li>
</ul>
<p>注意，交叉验证也是需要留出测试集的。比如，下图中留出了20%的数据当测试集，而其余80%的数据作为训练集+测试集进行5折交叉验证。</p>
<img src="C:\Users\11097\AppData\Roaming\Typora\typora-user-images\image-20230226221617505.png" alt="image-20230226221617505" style="zoom:50%;" />



<h1 id="CH3-Linear-Model"><a href="#CH3-Linear-Model" class="headerlink" title="CH3 Linear Model"></a>CH3 Linear Model</h1><p>顾名思义，指在监督式学习中只采用线性函数，即采用：<br>$$<br>f(\vec{x}) &#x3D; \vec{w} \cdot \vec{x} + b<br>$$<br>为了方便，我们定义<br>$$<br>\vec{w’} :&#x3D; \begin{bmatrix}<br>b \<br>w_1 \<br>\vdots \<br>w_p<br>\end{bmatrix},<br>\vec{x_i’} :&#x3D; \begin{bmatrix}<br>1 \<br>x_{i1} \<br>\vdots \<br>x_{ip}<br>\end{bmatrix}<br>$$<br>于是<br>$$<br>f(\vec{x’}) &#x3D; \vec{w’} \cdot \vec{x’}<br>$$<br>我们也可以定义一个所谓的design matrix<br>$$<br>X :&#x3D; \begin{bmatrix}<br>1 &amp; x_{11} &amp; \cdots &amp; x_{1p} \<br>1 &amp; x_{21} &amp; \cdots &amp; x_{2p} \<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>1 &amp; x_{n1} &amp; \cdots &amp; x_{np} \<br>\end{bmatrix}<br>$$<br>我们记<br>$$<br>Y :&#x3D; \begin{bmatrix}<br>y_1 \<br>y_2 \<br>\vdots \<br>y_p<br>\end{bmatrix}<br>$$</p>
<p>以上就是线性模型的基本框架。</p>
<p>线性模型是最简单的模型之一，通常只能解决线性问题。不过，对于非线性问题，可以通过<strong>线性模型+神经网络</strong>或者<strong>线性模型+映射</strong>来解决，后两者也是很常用的解决非线性问题的方法。核函数方法就属于后者。</p>
<p>有一个小区别需要注意一下：按照线性代数中对线性变换的定义，一维情况下，$y&#x3D;ax$是线性变换，$y&#x3D;ax+b$不是线性变换，即不能有偏置项。高维同理。也许把$\vec{w}, \vec{x}$重新定义成$\vec{w’}, \vec{x’}$的除了方便外的另一个原因是和线代统一。</p>
<h2 id="Normal-Linear-Regression"><a href="#Normal-Linear-Regression" class="headerlink" title="Normal Linear Regression"></a>Normal Linear Regression</h2><p>以下为了方便，我把$\vec{w’}, \vec{x’}$写为$\vec{w}, \vec{x}$。</p>
<p>在普通线性回归中，model是<br>$$<br>f(\vec{x}) &#x3D; \vec{w} \cdot \vec{x}<br>$$<br>以下所有方法中，我都将用$E$表示要优化的目标函数。$E$这个字母很有意思，统计出身的做机器学习的通常把它理解成$Error$，而物理出身的做机器学习的通常把它理解成$Energy$，从而和统计物理扯上关系。也有很多学者把E称为loss function。我们把$E$理解成要优化的目标函数就好了。</p>
<p>在普通线性回归中，误差定义为<br>$$<br>E(\vec{w}) :&#x3D; \sum_{i&#x3D;1}^{n}(y_i-\vec{w} \cdot \vec{x_i})^2<br>$$<br><strong>如果按上述公式定义误差，那么普通线性回归和最小二乘法就是一个东西。</strong></p>
<p>Dataset给定时，$E$仅是$\vec{w}$的函数。它是一个凸函数，可以求一阶导数得到全局极小。<strong>（注意，在所有方法中，优化E时Dataset都是已知的，即$(\vec{x_1},y_1), (\vec{x_2},y_2),…,(\vec{x_n},y_n)$都已知）</strong></p>
<h2 id="Generalized-Linear-Model"><a href="#Generalized-Linear-Model" class="headerlink" title="Generalized Linear Model"></a>Generalized Linear Model</h2><p>在GLM中<br>$$<br>f(\vec{x}) &#x3D; g^{-1}(\vec{w} \cdot \vec{x})<br>$$</p>
<p>当$g(x) &#x3D; ln(x)$时，我们称之为log linear regression (对数线性回归&#x2F;对数线性模型)<br>$$<br>f(\vec{x}) &#x3D; e^{\vec{w} \cdot \vec{x}}<br>$$</p>
<p>注意，对数线性回归和Logistic Regression（对数几率回归）不是一个东西，后者引入了logit（几率），前者没有。</p>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Generalized_linear_model#Logit_link_function">广义线性模型的维基百科页面</a>写得也很不错，但对考试估计帮助不大，考完试再。</p>
<h2 id="From-Regression-to-Classification"><a href="#From-Regression-to-Classification" class="headerlink" title="From Regression to Classification"></a>From Regression to Classification</h2><p>普通线性回归、对数线性回归、广义线性回归都用来解决回归问题。那么怎么用线性模型做分类问题呢？</p>
<p>有两种方法，第一种方法是在这三种方法的基础上，把曲线一侧的划为一类，另一侧的划为另一类，如下图所示。我在lab1中尝试了一下普通线性回归+这种方法，效果还不错，准确率和Logistic Regression差不多（就是我直接用pinv函数，你跟我说这是耍赖的那次）。</p>
<img src="C:\Users\11097\AppData\Roaming\Typora\typora-user-images\image-20230227212715725.png" alt="image-20230227212715725" style="zoom:50%;" />

<p>第二种方法就是Logistic Regression咯，这种方法给出了一种概率含义，详情见下。</p>
<h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><p>In LR,<br>$$<br>E(\vec{w}) :&#x3D; \sum_{i&#x3D;1}^{n} (-y_i \vec{w} \cdot \vec{x_i} + ln(1+e^{\vec{w} \cdot \vec{x_i}})), when \ y \in {0,+1}<br>$$</p>
<p>It’s hard to calculate $\frac{\partial E}{\partial \vec{w}}$ (According to Wikipedia, it doesn’t have a close-form solution), so we use numerical method like gradient descent and Newton method to get the optimal solution $\hat w$.</p>
<p>E的推导和LR的概率含义见附录。</p>
<h2 id="LASSO-and-Ridge-Regression"><a href="#LASSO-and-Ridge-Regression" class="headerlink" title="LASSO and Ridge Regression"></a>LASSO and Ridge Regression</h2><p>为了防止过拟合，我们可以给LS和LR加上惩罚项。惩罚项通过控制$\vec{w}$的范数来降低模型复杂度。</p>
<p>LS+LASSO<br>$$<br>E(\vec{w}) :&#x3D; \sum_{i&#x3D;1}^{n}(y_i-\vec{w} \cdot \vec{x_i})^2 + \lambda||\vec{w}||<em>1^1<br>$$<br>LS+Ridge<br>$$<br>E(\vec{w}) :&#x3D; \sum</em>{i&#x3D;1}^{n}(y_i-\vec{w} \cdot \vec{x_i})^2 + \lambda||\vec{w}||<em>2^2<br>$$<br>LR+LASSO<br>$$<br>E(\vec{w}) :&#x3D; \sum</em>{i&#x3D;1}^{n} (-y_i \vec{w} \cdot \vec{x_i} + ln(1+e^{\vec{w} \cdot \vec{x_i}})) + \lambda||\vec{w}||<em>1^1<br>$$<br>LR+Ridge<br>$$<br>E(\vec{w}) :&#x3D; \sum</em>{i&#x3D;1}^{n} (-y_i \vec{w} \cdot \vec{x_i} + ln(1+e^{\vec{w} \cdot \vec{x_i}})) + \lambda||\vec{w}||_2^2<br>$$</p>
<p>把惩罚项显式地写出来（注意Ridge Regression的惩罚项是L2范数的平方，别把平方丢了。）</p>
<p>$$<br>||\vec{w}||<em>1^1 &#x3D; \sum</em>{j&#x3D;1}^{p} |w_j|<br>$$</p>
<p>$$<br>||\vec{w}||<em>2^2 &#x3D; \sum</em>{j&#x3D;1}^{p}w_j^2<br>$$</p>
<h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><p>PCA是K.P. Pearson发明的老古董。</p>
<p>PCA的思想：把样本从高维空间投影到低维空间，使得在低维空间中所有样本尽可能分开。</p>
<p>做到这一点的两个思路</p>
<ul>
<li>最小化所有样本到低维空间的总距离</li>
<li>最大化所有样本在低维空间的方差</li>
</ul>
<p>这两个思路是等价的（其实就是勾股定理）。<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=FgakZw6K1QQ">可以看看这个简单直观的YouTube视频</a>。</p>
<p>在PCA中，优化问题见(10.15)或者(10.16)。</p>
<p>利用拉格朗日乘数法，我们可以得到PCA的一个简单的算法：求$X^T X$的所有特征值，从大到小排序，取前$d’$个特征值，它们的特征向量组成了一个$d’$维的低维空间。通常，特征值的大小代表了重要程度。</p>
<p>$d’$也可以取为p，这时我们相当于我们做了一个同维度的线性变换。比如，在lab5中，直接调库会得到$PC_1, PC_2,…,PC_{120}$。</p>
<h2 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h2><p>和PCA类似，LDA是R.A. Fisher发明的老古董。</p>
<p>LDA的思想：把样本从高维空间投影到低维空间，使得在低维空间中同类样本尽可能近且异类样本尽可能远。</p>
<p>在LDA中，优化问题见(3.36)</p>
<p><strong>LASSO、Ridge Regression、PCA、LDA都可以用来做降维or特征选择。</strong></p>
<h2 id="Supporting-Vector-Machine"><a href="#Supporting-Vector-Machine" class="headerlink" title="Supporting Vector Machine"></a>Supporting Vector Machine</h2><p>SVM你学得比我懂，我这里只列出一些重点：</p>
<ul>
<li>原优化问题</li>
<li>对偶优化问题<ul>
<li>SMO算法</li>
</ul>
</li>
<li>软间隔SVM：相当于硬间隔SVM+惩罚项</li>
<li>核函数：使得SVM可以处理非线性问题（PCA、LDA也可以用类似的核函数方法）</li>
<li>SVR：把SVM用于回归</li>
</ul>
<p>PS：离SVM的分类直线最近的向量称为支持向量，这是SVM名字的来源。</p>
<h2 id="二分类拓展到多分类"><a href="#二分类拓展到多分类" class="headerlink" title="二分类拓展到多分类"></a>二分类拓展到多分类</h2><p>三种策略，各有优缺点</p>
<ul>
<li>one vs one</li>
<li>one vs rest</li>
<li>many vs many</li>
</ul>
<h2 id="类别不平衡"><a href="#类别不平衡" class="headerlink" title="类别不平衡"></a>类别不平衡</h2><p>这个考试应该没法考，但是在实际中是一个很重要的事情，我们在实验室里也需要考虑这个问题。</p>
<p>见书3.6节，书写得还是比较清楚的。</p>
<p>我在lab1中试了一下，lab1中类别不平衡不是特别显著，所以考不考虑影响不大。</p>
<h1 id="CH4-Decision-Tree"><a href="#CH4-Decision-Tree" class="headerlink" title="CH4 Decision Tree"></a>CH4 Decision Tree</h1><p>这一章不难，我迅速带过。</p>
<ul>
<li><p>我们希望决策树得分支节点所包含的样本尽可能属于同一类别，为此我们需要定量衡量一个集合的纯度。</p>
<ul>
<li><p>Shannon Entropy</p>
</li>
<li><p>Gini index</p>
</li>
</ul>
</li>
<li><p>有3个主流的算法</p>
<ul>
<li><p>ID3：用香农熵增益来选择属性</p>
</li>
<li><p>C4.5：用香农熵增益率来选择属性</p>
</li>
<li><p>CART：用基尼指数来选择属性</p>
</li>
</ul>
</li>
<li><p>剪枝：为了防止过拟合</p>
<ul>
<li>pre-pruning</li>
<li>post-pruning</li>
</ul>
</li>
</ul>
<p>PS：决策树是贪心算法——每次只选择对当前最有利的划分。</p>
<h1 id="CH5-Neural-Network"><a href="#CH5-Neural-Network" class="headerlink" title="CH5 Neural Network"></a>CH5 Neural Network</h1><ul>
<li>MP神经元模型: 即$y&#x3D;f(\vec{w} \cdot \vec{x} + b)$<ul>
<li>$f$为激活函数，通常为ReLU或者Sigmoid</li>
<li>b可以为0</li>
</ul>
</li>
<li>感知机：只有输入层、输出层的神经网络<ul>
<li>如果选择Sigmoid为激活函数，那么感知机和Logistic Regression的公式一模一样</li>
<li>感知机、LR都只能处理线性问题。</li>
<li>只有加入隐藏层，Neural Network才能处理非线性问题。</li>
<li>历史上，BP算法发明前，由于感知机只能处理线性问题且加入隐藏层之后的神经网络没法训练，神经网络陷入冰河期。</li>
</ul>
</li>
<li>前馈神经网络<ul>
<li>$E$见（5.4）</li>
<li>优化E的方法：梯度下降+BP算法</li>
<li>通常用随机梯度下降来避免陷入局部极小。</li>
</ul>
</li>
</ul>
<p>这一章的内容参考我之前发给的文件。</p>
<h1 id="CH8-Ensemble-Learning"><a href="#CH8-Ensemble-Learning" class="headerlink" title="CH8 Ensemble Learning"></a>CH8 Ensemble Learning</h1><ul>
<li><p>有两种方法</p>
<ul>
<li><p>bagging</p>
<ul>
<li>并行：基学习器之间不存在强依赖关系</li>
<li>eg：随机森林</li>
</ul>
</li>
<li><p>boosting</p>
<ul>
<li>串行：后一个基学习器依赖于前一个基学习器的结果</li>
<li>eg: AdaBoost, XGboost</li>
</ul>
</li>
</ul>
</li>
<li><p>投票：见8.4</p>
</li>
<li><p>集成学习的理论分析：见8.5，指出了集成学习如何实现“好而不同”</p>
</li>
</ul>
<h1 id="CH9-Clustering"><a href="#CH9-Clustering" class="headerlink" title="CH9 Clustering"></a>CH9 Clustering</h1><ul>
<li>聚类是一种无监督学习，也是这本书里唯一的一个无监督学习</li>
<li>聚类的性能度量（类似于监督式学习中的$E$）：见9.2</li>
<li>选择合适的距离：见9.3</li>
<li>具体方法<ul>
<li>prototype-based clustering：用一组原型刻画，基于原型聚类<ul>
<li>K-means</li>
<li>Learning Vector Quantization</li>
<li>高斯混合模型</li>
</ul>
</li>
<li>density-based clustering：基于密度聚类<ul>
<li>DBSCAN</li>
<li>缺点：只能聚球形的</li>
</ul>
</li>
<li>层次聚类：我们在数据结构里学过类似的东西</li>
<li>lab4（这个方法真的很帅）</li>
</ul>
</li>
</ul>
<h1 id="CH10-Dimensionality-Reduction"><a href="#CH10-Dimensionality-Reduction" class="headerlink" title="CH10 Dimensionality Reduction"></a>CH10 Dimensionality Reduction</h1><p>特征选择和降维是解决维度诅咒的两个方法。</p>
<p>做过lab5的你对CH10和CH11一定印象深刻，我就列个目录了。它们也都不难，耐心看。</p>
<ul>
<li>Multiple Dimensional Scaling</li>
<li>PCA</li>
<li>Kernelized PCA</li>
<li>Manifold Learning<ul>
<li>Isomap</li>
<li>LLE</li>
</ul>
</li>
<li>Metric Learning</li>
</ul>
<h1 id="CH11-Feature-Selection"><a href="#CH11-Feature-Selection" class="headerlink" title="CH11 Feature Selection"></a>CH11 Feature Selection</h1><ul>
<li>过滤法<ul>
<li>RELIEF</li>
</ul>
</li>
<li>包裹法<ul>
<li>LVW</li>
</ul>
</li>
<li>嵌入法<ul>
<li>LASSO</li>
<li>决策树</li>
<li>随机森林</li>
</ul>
</li>
</ul>
<p>其实我感觉包裹法和嵌入法不用区分得太开</p>
<p>CH11最后介绍了字典学习和压缩感知，你在凸优化是应该学过后者。</p>
<h1 id="CH13-半监督学习"><a href="#CH13-半监督学习" class="headerlink" title="CH13 半监督学习"></a>CH13 半监督学习</h1><ul>
<li>需求：我们需要利用没有标签的样本，这个需求在现实中很常见<ul>
<li>主动学习：让人来判断</li>
<li>半监督学习：不用人来判断</li>
</ul>
</li>
<li>半监督学习（不是很难，耐心看）<ul>
<li>生成式模型</li>
<li>半监督SVM</li>
<li>图半监督学习</li>
<li>disagreement based methods</li>
<li>半监督聚类</li>
</ul>
</li>
</ul>
<h1 id="CH7-Bayes"><a href="#CH7-Bayes" class="headerlink" title="CH7 Bayes"></a>CH7 Bayes</h1><ul>
<li>生成式模型vs判别式模型：P148</li>
<li>假设有p个特征<ul>
<li>not naive Bayes需要的数据正比于$2^{p}$</li>
<li>naive Bayes需要的数据正比于$p$</li>
</ul>
</li>
<li>naive Bayes<ul>
<li>引入属性条件独立假设的贝叶斯分类器称为naive Bayes</li>
<li>没办法，数据就这么多，只能假设条件独立</li>
<li>拉普拉斯平滑：给每个小格子加上1</li>
</ul>
</li>
<li>semi naive Bayes<ul>
<li>在现实中，属性条件独立假设难以成立，于是人们开发出半朴素贝叶斯</li>
<li>One Dependent：每个属性仅仅依赖于1个属性</li>
</ul>
</li>
<li>Bayes Network：不难，耐心看，见7.5</li>
<li>EM算法：老师说必考，见7.6</li>
</ul>
<h1 id="CH14-概率图模型"><a href="#CH14-概率图模型" class="headerlink" title="CH14 概率图模型"></a>CH14 概率图模型</h1><p>这一章确实较难，但是也确实很有用，我们实验室的几个师兄经常用。</p>
<p>CH14我学得不是很懂，但我相信你能看懂~</p>
<h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="Normal-Linear-Regression的解析解"><a href="#Normal-Linear-Regression的解析解" class="headerlink" title="Normal Linear Regression的解析解"></a>Normal Linear Regression的解析解</h2><p>在普通线性回归中，E的定义如下<br>$$<br>E(\vec{w}) :&#x3D; \sum_{i&#x3D;1}^{n}(y_i-\vec{w} \cdot \vec{x_i})^2 &#x3D; Y^TY + w^TX^TXw - 2w^TX^TY<br>$$</p>
<p>求一阶导，我们得到<br>$$<br>\frac{\partial E}{\partial \vec{w}} &#x3D; 2(X^TXw-X^TY)<br>$$<br>于是<br>$$<br>\frac{\partial E}{\partial \vec{w}} &#x3D; 0 \Rightarrow X^TX \hat w &#x3D; X^TY<br>$$<br>当$X^TX$可逆时<br>$$<br>\hat w &#x3D; (X^TX)^{-1}X^TY &#x3D; X^\dagger Y<br>$$<br>where $X^\dagger :&#x3D; (X^TX)^{-1}X^T$ and  $X^\dagger$ is Moore-Penrose inverse, as we learned in YYN’s class.</p>
<p>当$X^TX$不可逆时，$X^TX \hat w &#x3D; X^TY$这个线性方程组有多个解，选择哪一个作为结果由算法的偏好决定。</p>
<p>线性代数中有一个很有名的定理，杨亚宁也讲过：$X列满秩 \Leftrightarrow X^{T}X 可逆$</p>
<p>$X$的行数越多，线性无关的行向量<strong>很可能</strong>越多。线性无关的行向量越多，行秩、秩、列秩越大，X越容易列满秩。</p>
<p>以下是三个例子：</p>
<ul>
<li><strong>在lab5中，X有10000行和121列，X大概率列满秩。</strong></li>
<li><strong>在波士顿房价数据集中，X有506行和14列，X大概率列满秩。</strong></li>
<li>在生物信息学中，通常feature数目比sample多，X大概率列不满秩。</li>
</ul>
<h2 id="Logistic-Regression的概率含义和E的推导"><a href="#Logistic-Regression的概率含义和E的推导" class="headerlink" title="Logistic Regression的概率含义和E的推导"></a>Logistic Regression的概率含义和E的推导</h2><p>LR中，我们假设<br>$$<br>\begin{Bmatrix}<br>P(y&#x3D;+1|\vec{x},\vec{w},b) &#x3D; \frac{1}{1+e^{- \vec{w} \cdot \vec{x} }} \<br>P(y&#x3D;0|\vec{x},\vec{w},b) &#x3D; \frac{1}{1+e^{+ \vec{w} \cdot \vec{x} }}<br>\end{Bmatrix}<br>$$<br>注意，这是一个假设。</p>
<p>正是这个假设赋予了LR概率含义。$P(y&#x3D;+1|\vec{x},\vec{w},b)$即样本$\vec{x}$是正例的概率，$P(y&#x3D;0|\vec{x},\vec{w},b)$即样本$\vec{x}$是反例的概率。如下图所示，离分类直线越远，属于对应类的概率越大。</p>
<img src="C:\Users\11097\AppData\Roaming\Typora\typora-user-images\image-20230227232313535.png" alt="image-20230227232313535" style="zoom:50%;" />

<p>接下来推导E：</p>
<p>把伯努利分布的两个P写到一起<br>$$<br>P(y_i|\vec{x_i},\vec{w},b) &#x3D; (P(y_i &#x3D; +1|\vec{x_i},\vec{w},b))^{y_i} (P(y_i &#x3D; 0|\vec{x_i},\vec{w},b))^{1 - y_i}<br>$$<br>似然函数为<br>$$<br>likelihood(Y|X,\vec{w},b) &#x3D; \prod_{i&#x3D;1}^{n} P(y_i|\vec{x_i},\vec{w},b)<br>$$<br>对数似然函数为<br>$$<br>ln(likelihood(Y|X,\vec{w},b)) &amp; &#x3D; &amp; \sum_{i&#x3D;1}^{n} ln(P(y_i|\vec{x_i},\vec{w},b)) \<br>&amp; &#x3D; &amp; \sum_{i&#x3D;1}^{n} y_i \vec{w} \cdot \vec{x_i} - ln(1+e^{\vec{w} \cdot \vec{x_i}})<br>$$<br>最大化对数似然函数等价于最小化负对数似然函数，因此我们可以把$E$定义为<br>$$<br>E :&#x3D; \sum_{i&#x3D;1}^{n} -  y_i \vec{w} \cdot \vec{x_i} + ln(1+e^{\vec{w} \cdot \vec{x_i}})<br>$$</p>
<h2 id="懒惰学习-vs-急切学习"><a href="#懒惰学习-vs-急切学习" class="headerlink" title="懒惰学习 vs 急切学习"></a>懒惰学习 vs 急切学习</h2><p>kNN是我们学过的唯一一个懒惰学习方法，其它全都是急切学习。</p>
<p>CH7提到了一次懒惰学习，但是没有展开。</p>
<h2 id="一些常见的数据集"><a href="#一些常见的数据集" class="headerlink" title="一些常见的数据集"></a>一些常见的数据集</h2><ul>
<li>Iris: R.A. Fisher玩鸢尾时候留下的数据集</li>
<li>handwritten digits: 手写数字识别，很适合神经网络</li>
<li>波士顿房价数据集：用线性模型即可</li>
<li>泰坦尼克号数据集：适合练练特征选择</li>
</ul>
<p>还有很多toy dataset，见sklearn的官方说明文档。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/02/25/The_Biggest_Meet_Eating_Dinosaur/" rel="prev" title="最大的食肉恐龙">
                  <i class="fa fa-angle-left"></i> 最大的食肉恐龙
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/03/01/Welcome_to_Wenlab/" rel="next" title="Welcome to Wenlab">
                  Welcome to Wenlab <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Physics-Lee</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Word count total">201k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">11:12</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdnjs.cloudflare.com/ajax/libs/pdfobject/2.2.12/pdfobject.min.js","integrity":"sha256-g2xji1rlE3KsGVClvuxTbcR0Kn2+wtQADSff2Tbb4zA="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="/js/third-party/tags/pdf.js"></script>






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




  <script async src="/js/cursor/fireworks.js"></script>




  <script src="/js/type/type.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"Physics-Lee","repo":"physics-lee.github.io","client_id":"afcfae9bf2a88299fba8","client_secret":"52d3480f8f93d03a5c970c34934701b527e68b38","admin_user":"Physics-Lee","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"en","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"4b2e6ca24213ab02e42ee237b7f6fba4"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
